{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30512,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "======================================================================================================\n",
        "\n",
        "**Disclaimer**: Parts of this notebook are adapted from this [**open source notebook**](https://www.kaggle.com/code/auxeno/dqn-on-lunar-lander-rl)  with minor modifications. Credit to the original authors.\n",
        "\n",
        "======================================================================================================"
      ],
      "metadata": {
        "id": "J170wQepg54F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop 5\n",
        "## Deep Q-Learning Gym's Lunar Lander Environment\n",
        "\n",
        "In this notebook, we will explore the implementation of a Deep Q-Learning (DQN) agent to navigate Gym's Lunar Lander environment.\n",
        "\n",
        "In the Lunar Lander environment, the agent's task is to learn how to land a lunar module safely on the moon's surface. This requires the agent to balance fuel efficiency and safety considerations. The agent needs to learn from its past experiences, developing a strategy to approach the landing pad while minimizing its speed and using as little fuel as possible.\n",
        "\n",
        "Let's initialize a LunarLander-v3 environmnet, make random actions in the environment, then view a recording of it."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-06-21T19:20:26.339215Z",
          "iopub.execute_input": "2023-06-21T19:20:26.339632Z",
          "iopub.status.idle": "2023-06-21T19:20:26.345129Z",
          "shell.execute_reply.started": "2023-06-21T19:20:26.3396Z",
          "shell.execute_reply": "2023-06-21T19:20:26.343795Z"
        },
        "id": "Cc1PJ_41fZyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "Zh7WWoS7jFv0",
        "outputId": "c745af01-ba44-4b35-a560-ac8aaea5c420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install pyglet > /dev/null 2>&1\n",
        "!pip install swig > /dev/null 2>&1\n",
        "!pip install \"gymnasium[all]\" pygame matplotlib numpy pandas > /dev/null 2>&1"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T07:44:29.965630Z",
          "iopub.execute_input": "2025-02-26T07:44:29.965953Z",
          "iopub.status.idle": "2025-02-26T07:45:23.976913Z",
          "shell.execute_reply.started": "2025-02-26T07:44:29.965927Z",
          "shell.execute_reply": "2025-02-26T07:45:23.975803Z"
        },
        "trusted": true,
        "id": "XuUz8exofZy0"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay moviepy > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "uFrXpCBcgXMj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import ImageSequenceClip\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def display_video_from_frames(frames, fps=30, video_filename=\"lunar_lander.mp4\", width=700):\n",
        "    \"\"\"\n",
        "    Converts a list of frames into a video and displays it within a Jupyter Notebook or Google Colab.\n",
        "\n",
        "    Parameters:\n",
        "    - frames: List of frames (numpy arrays) to be compiled into a video.\n",
        "    - fps: Frames per second for the output video.\n",
        "    - video_filename: Name of the output video file.\n",
        "    - width: Width of the displayed video in pixels.\n",
        "\n",
        "    Returns:\n",
        "    - An IPython.display.HTML object that embeds the video.\n",
        "    \"\"\"\n",
        "    # Create video clip\n",
        "    clip = ImageSequenceClip(frames, fps=fps)\n",
        "\n",
        "    # Write the video to a file\n",
        "    clip.write_videofile(video_filename, codec=\"libx264\")\n",
        "\n",
        "    # Read the video file and encode it to base64\n",
        "    with open(video_filename, 'rb') as f:\n",
        "        video_data = f.read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "    # Return the HTML object to display the video\n",
        "    return HTML(f\"\"\"\n",
        "\n",
        "    \"\"\")\n"
      ],
      "metadata": {
        "id": "FeMoKNbHgbzt",
        "outputId": "209ce796-6080-4ef7-9763-cdf736b3d4de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('LunarLander-v3',render_mode='rgb_array')\n",
        "env.reset(seed=42)\n",
        "\n",
        "frames=[]\n",
        "\n",
        "# Play one complete episode with random actions\n",
        "while True:\n",
        "    frames.append(env.render())\n",
        "    action = env.action_space.sample()\n",
        "    _, _, terminated, truncated, _ = env.step(action)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "\n",
        "display_video_from_frames(frames)\n",
        "env.close()"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2025-02-26T07:45:23.979218Z",
          "iopub.execute_input": "2025-02-26T07:45:23.979663Z",
          "iopub.status.idle": "2025-02-26T07:45:24.823107Z",
          "shell.execute_reply.started": "2025-02-26T07:45:23.979628Z",
          "shell.execute_reply": "2025-02-26T07:45:24.821851Z"
        },
        "trusted": true,
        "id": "grCk_L-HfZy2",
        "outputId": "bb16d770-4a59-4547-be98-e3bd810eb31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video lunar_lander.mp4.\n",
            "Moviepy - Writing video lunar_lander.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready lunar_lander.mp4\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Deep Reinforcement Learning\n",
        "To address this challenge, we'll use deep reinforcement learning techniques to train an agent to land the spacecraft.\n",
        "\n",
        "Simpler tabular methods are limited to discrete observation spaces, meaning there are a finite number of possible states. In `LunarLander-v2` however, we're dealing with a continuous range of states across 8 different parameters, meaning there are a near-infinite number of possible states. We could try to bin similar values into groups, but due to the sensitive controls of the game, even slight errors can lead to significant missteps.\n",
        "\n",
        "To get around this, we'll use a `neural network Q-function approximator`. This lets us predict the best actions to take for a given state, even when dealing with a vast number of potential states. It's a much better match for our complex landing challenge.\n",
        "\n",
        "## The DQN Algorithm:\n",
        "\n",
        "This breakthrough algorithm was used by Mihn et al in 2015 to achieve human-level performance on several Atari 2600 games.\n",
        "\n",
        "The original paper published in Nature can be viewed here:\n",
        "\n",
        "https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning\n",
        "\n",
        "The algorithm:\n",
        "\n",
        "1. **Initialization**: Begin by initializing the parameters for two neural networks, $Q(s,a)$ (referred to as the online network) and $\\hat{Q}(s,a)$ (known as the target network), with random weights. Both networks serve the function of mapping a state-action pair to a Q-value, which is an estimate of the expected return from that pair. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer to store past transition experiences.\n",
        "2. **Action Selection**: Utilize an epsilon-greedy strategy for action selection. With a probability of $\\epsilon$, select a random action $a$, but in all other instances, choose the action $a$ that maximizes the Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
        "3. **Experience Collection**: Execute the chosen action $a$ within the environment emulator and observe the resulting immediate reward $r$ and the next state $s'$.\n",
        "4. **Experience Storage**: Store the transition $(s,a,r,s')$ in the replay buffer for future reference.\n",
        "5. **Sampling**: Randomly sample a mini-batch of transitions from the replay buffer for training the online network.\n",
        "6. **Target Computation**: For every transition in the sampled mini-batch, compute the target value $y$. If the episode has ended at this step, $y$ is simply the reward $r$. Otherwise, $y$ is the sum of the reward and the discounted estimated optimal future Q-value, i.e.,  $y = r + \\gamma \\max_{a' \\in A} \\hat{Q}(s', a')$\n",
        "7. **Loss Calculation**: Compute the loss, which is the squared difference between the Q-value predicted by the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$\n",
        "8. **Online Network Update**: Update the parameters of the online network $Q(s,a)$ using Stochastic Gradient Descent (SGD) to minimize the loss.\n",
        "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network to the target network $\\hat{Q}(s,a)$.\n",
        "10. **Iterate**: Repeat the process from step 2 until convergence.\n",
        "\n",
        "### Defining the Deep Q-Network\n",
        "Our network will be a simple feedforward neural network that takes the state as input and produces Q-values for each action as output. For `LunarLander-v2` the state is an 8-dimensional vector and there are 4 possible actions.\n"
      ],
      "metadata": {
        "id": "IlMPvk9IfZy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class DQN(torch.nn.Module):\n",
        "    '''\n",
        "    This class defines a deep Q-network (DQN), a type of artificial neural network used in reinforcement learning.\n",
        "    The DQN is used to estimate the Q-values, which represent the expected return for each action in each state.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state_size: int, default=8\n",
        "        The size of the state space.\n",
        "    action_size: int, default=4\n",
        "        The size of the action space.\n",
        "    hidden_size: int, default=64\n",
        "        The size of the hidden layers in the network.\n",
        "    '''\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
        "        '''\n",
        "        Initialize a network with the following architecture:\n",
        "            Input layer (state_size, hidden_size)\n",
        "            Hidden layer 1 (hidden_size, hidden_size)\n",
        "            Output layer (hidden_size, action_size)\n",
        "        '''\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        '''\n",
        "        Define the forward pass of the DQN. This function is called when the network is called to estimate Q-values.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: torch.Tensor\n",
        "            The state for which to estimate the Q-values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The estimated Q-values for each action in the input state.\n",
        "        '''\n",
        "        x = torch.relu(self.layer1(state))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T07:45:24.824266Z",
          "iopub.execute_input": "2025-02-26T07:45:24.824532Z",
          "iopub.status.idle": "2025-02-26T07:45:28.702527Z",
          "shell.execute_reply.started": "2025-02-26T07:45:24.824510Z",
          "shell.execute_reply": "2025-02-26T07:45:28.701271Z"
        },
        "trusted": true,
        "id": "DMmFzU07fZy4"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Replay Buffer\n",
        "In the context of RL, we employ a structure known as the replay buffer, which utilizes a deque. The replay buffer stores and samples experiences, which helps us overcome the problem of *step correlation*.\n",
        "\n",
        "A *deque* (double-ended queue) is a data structure that enables the addition or removal of elements from both its ends, hence the name. It is particularly useful when there is a need for fast append and pop operations from either end of the container, which it provides at O(1) time complexity. In contrast, a list offers these operations at O(n) time complexity, making the deque a preferred choice in cases that necessitate more efficient operations.\n",
        "\n",
        "Moreover, a deque allows setting a maximum size. Once this maximum size is exceeded during an insertion (push) operation at the front, the deque automatically ejects the item at the rear, thereby maintaining its maximum length.\n",
        "\n",
        "In the replay buffer, the `push` method is utilized to add an experience. If adding this experience exceeds the maximum buffer size, the oldest (rear-most) experience is automatically removed. This approach ensures that the replay buffer always contains the most recent experiences up to its capacity.\n",
        "\n",
        "The `sample` method, on the other hand, is used to retrieve a random batch of experiences from the replay buffer. This randomness is critical in breaking correlations within the sequence of experiences, which leads to more robust learning.\n",
        "\n",
        "This combination of recency and randomness allows us to learn on new training data, without training samples being highly correlated."
      ],
      "metadata": {
        "id": "2crCm50sfZy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    '''\n",
        "    This class represents a replay buffer, a type of data structure commonly used in reinforcement learning algorithms.\n",
        "    The buffer stores past experiences in the environment, allowing the agent to sample and learn from them at later times.\n",
        "    This helps to break the correlation of sequential observations and stabilize the learning process.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    buffer_size: int, default=10000\n",
        "        The maximum number of experiences that can be stored in the buffer.\n",
        "    '''\n",
        "    def __init__(self, buffer_size=10000):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        '''\n",
        "        Add a new experience to the buffer. Each experience is a tuple containing a state, action, reward,\n",
        "        the resulting next state, and a done flag indicating whether the episode has ended.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The state of the environment before taking the action.\n",
        "        action: int\n",
        "            The action taken by the agent.\n",
        "        reward: float\n",
        "            The reward received after taking the action.\n",
        "        next_state: array-like\n",
        "            The state of the environment after taking the action.\n",
        "        done: bool\n",
        "            A flag indicating whether the episode has ended after taking the action.\n",
        "        '''\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Randomly sample a batch of experiences from the buffer. The batch size must be smaller or equal to the current number of experiences in the buffer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            The number of experiences to sample from the buffer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of numpy.ndarray\n",
        "            A tuple containing arrays of states, actions, rewards, next states, and done flags.\n",
        "        '''\n",
        "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Get the current number of experiences in the buffer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The number of experiences in the buffer.\n",
        "        '''\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T07:45:28.704053Z",
          "iopub.execute_input": "2025-02-26T07:45:28.704770Z",
          "iopub.status.idle": "2025-02-26T07:45:28.712387Z",
          "shell.execute_reply.started": "2025-02-26T07:45:28.704743Z",
          "shell.execute_reply": "2025-02-26T07:45:28.711073Z"
        },
        "trusted": true,
        "id": "LNRo-DkbfZy5"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the DQN Agent\n",
        "The DQN agent handles the interaction with the environment, selecting actions, collecting experiences, storing them in the replay buffer, and using these experiences to train the network. Let's walk through each part of this process:\n",
        "\n",
        "#### Initialisation\n",
        "The `__init__` function sets up the agent:\n",
        "\n",
        "- `self.device`: We start by checking whether a GPU is available, and, if so, we use it, otherwise, we fall back to CPU.\n",
        "- `self.gamma`: This is the discount factor for future rewards, used in the Q-value update equation.\n",
        "- `self.batch_size`: This is the number of experiences we'll sample from the memory when updating the model.\n",
        "- `self.q_network` and `self.target_network`: These are two instances of the Q-Network. The first is the network we're actively training, and the second is a copy that gets updated less frequently. This helps to stabilize learning.\n",
        "- `self.optimizer`: This is the optimization algorithm used to update the Q-Network's parameters.\n",
        "- `self.memory`: This is a replay buffer that stores experiences. It's an instance of the `ReplayBuffer` class.\n",
        "\n",
        "#### Step Function\n",
        "The `step` function is called after each timestep in the environment:\n",
        "\n",
        "- The function starts by storing the new experience in the replay buffer.\n",
        "- If enough experiences have been stored, it calls `self.update_model()`, which triggers a learning update.\n",
        "\n",
        "#### Action Selection\n",
        "The act function is how the agent selects an action:\n",
        "\n",
        "- If a randomly drawn number is greater than $\\epsilon$, it selects the action with the highest predicted Q-value. This is known as exploitation: the agent uses what it has learned to select the best action.\n",
        "- If the random number is less than $\\epsilon$, it selects an action randomly. This is known as exploration: the agent explores the environment to learn more about it.\n",
        "\n",
        "#### Model Update\n",
        "The `update_model` function is where the learning happens:\n",
        "\n",
        "- It starts by sampling a batch of experiences from the replay buffer.\n",
        "- It then calculates the current Q-values for the sampled states and actions, and the expected - Q-values based on the rewards and next states.\n",
        "- It calculates the loss, which is the mean squared difference between the current and expected Q-values.\n",
        "- It then backpropagates this loss through the Q-Network and updates the weights using the optimizer.\n",
        "\n",
        "#### Target Network Update\n",
        "Finally, the `update_target_network` function copies the weights from the Q-Network to the Target Network. This is done periodically (not every step), to stabilize the learning process. Without this, the Q-Network would be trying to follow a moving target, since it's learning from estimates produced by itself."
      ],
      "metadata": {
        "id": "ATEiQQczfZy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    '''\n",
        "    This class represents a Deep Q-Learning agent that uses a Deep Q-Network (DQN) and a replay memory to interact\n",
        "    with its environment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state_size: int, default=8\n",
        "        The size of the state space.\n",
        "    action_size: int, default=4\n",
        "        The size of the action space.\n",
        "    hidden_size: int, default=64\n",
        "        The size of the hidden layers in the network.\n",
        "    learning_rate: float, default=1e-3\n",
        "        The learning rate for the optimizer.\n",
        "    gamma: float, default=0.99\n",
        "        The discount factor for future rewards.\n",
        "    buffer_size: int, default=10000\n",
        "        The maximum size of the replay memory.\n",
        "    batch_size: int, default=64\n",
        "        The batch size for learning from the replay memory.\n",
        "    '''\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "        # Select device to train on (if CUDA available, use it, otherwise use CPU)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Discount factor for future rewards\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Batch size for sampling from the replay memory\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Number of possible actions\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Initialize the Q-Network and Target Network with the given state size, action size and hidden layer size\n",
        "        # Move the networks to the selected device\n",
        "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "\n",
        "        # Set weights of target network to be the same as those of the q network\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Set target network to evaluation mode\n",
        "        self.target_network.eval()\n",
        "\n",
        "        # Initialize the optimizer for updating the Q-Network's parameters\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Initialize the replay memory\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        '''\n",
        "        Perform a step in the environment, store the experience in the replay memory and potentially update the Q-network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The current state of the environment.\n",
        "        action: int\n",
        "            The action taken by the agent.\n",
        "        reward: float\n",
        "            The reward received after taking the action.\n",
        "        next_state: array-like\n",
        "            The state of the environment after taking the action.\n",
        "        done: bool\n",
        "            A flag indicating whether the episode has ended after taking the action.\n",
        "        '''\n",
        "        # Store the experience in memory\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "        # If there are enough experiences in memory, perform a learning step\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        '''\n",
        "        Choose an action based on the current state and the epsilon-greedy policy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The current state of the environment.\n",
        "        eps: float, default=0.\n",
        "            The epsilon for the epsilon-greedy policy. With probability eps, a random action is chosen.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The chosen action.\n",
        "        '''\n",
        "        # If a randomly chosen value is greater than eps\n",
        "        if random.random() > eps:\n",
        "            # Convert state to a PyTorch tensor and set network to evaluation mode\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "\n",
        "            # With no gradient updates, get the action values from the DQN\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "\n",
        "            # Revert to training mode and return action\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            # Return a random action for random value > eps\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        '''\n",
        "        Update the Q-network based on a batch of experiences from the replay memory.\n",
        "        '''\n",
        "        # Sample a batch of experiences from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        # Get Q-values for the actions that were actually taken\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Get maximum Q-value for the next states from target network\n",
        "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss between the current and expected Q values\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Zero all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        '''\n",
        "        Update the weights of the target network to match those of the Q-network.\n",
        "        '''\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T07:45:28.715410Z",
          "iopub.execute_input": "2025-02-26T07:45:28.715701Z",
          "iopub.status.idle": "2025-02-26T07:45:28.748336Z",
          "shell.execute_reply.started": "2025-02-26T07:45:28.715666Z",
          "shell.execute_reply": "2025-02-26T07:45:28.747402Z"
        },
        "trusted": true,
        "id": "ZHQWNR5sfZy6"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Agent\n",
        "\n",
        "Training the agent involves having the agent interact with the `LunarLander-v2` environment over a sequence of steps. Over each step, the agent receives a state from the environment, selects an action, receives a reward and the next state, and then updates its understanding of the environment (the Q-table in the case of Q-Learning).\n",
        "\n",
        "The `train` function orchestrates this process over a defined number of episodes, using the methods defined in the DQNAgent class. Here's how it works:\n",
        "\n",
        "#### Initial Setup\n",
        "- `scores`: This list stores the total reward obtained in each episode.\n",
        "- `scores_window`: This is a double-ended queue with a maximum length of 100. It holds the scores of the most recent 100 episodes and is used to monitor the agent's performance.\n",
        "-`eps`: This is the epsilon for epsilon-greedy action selection. It starts from `eps_start` and decays after each episode until it reaches `eps_end`.\n",
        "\n",
        "#### Episode Loop\n",
        "The training process runs over a fixed number of episodes. In each episode:\n",
        "\n",
        "- The environment is reset to its initial state.\n",
        "- he agent then interacts with the environment until the episode is done (when a terminal state is reached).\n",
        "\n",
        "#### Step Loop\n",
        "In each step of an episode:\n",
        "\n",
        "- The agent selects an action using the current policy (the act method in `DQNAgent`).\n",
        "The selected action is applied to the environment using the step method, which returns the next state, the reward, and a boolean indicating whether the episode is done.\n",
        "- The agent's step method is called to update the agent's knowledge. This involves adding the experience to the replay buffer and, if enough experiences have been collected, triggering a learning update.\n",
        "- The state is updated to the next state, and the reward is added to the score.\n",
        "\n",
        "After each episode:\n",
        "\n",
        "- The score for the episode is added to `scores` and `scores_window`.\n",
        "- Epsilon is decayed according to `eps_decay`.\n",
        "- If the episode is a multiple of `target_update`, the target network is updated with the latest weights from the Q-Network.\n",
        "- Finally, every 100 episodes, the average score over the last 100 episodes is printed.\n",
        "\n",
        "The function returns the list of scores for all episodes.\n",
        "\n",
        "This training process, which combines experiences from the replay buffer and separate target and Q networks, helps to stabilize the learning and leads to a more robust policy."
      ],
      "metadata": {
        "id": "XtqdvPVOfZy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(agent, env, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, target_update=10):\n",
        "    '''\n",
        "    Train a DQN agent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    agent: DQNAgent\n",
        "        The agent to be trained.\n",
        "    env: gym.Env\n",
        "        The environment in which the agent is trained.\n",
        "    n_episodes: int, default=2000\n",
        "        The number of episodes for which to train the agent.\n",
        "    eps_start: float, default=1.0\n",
        "        The starting epsilon for epsilon-greedy action selection.\n",
        "    eps_end: float, default=0.01\n",
        "        The minimum value that epsilon can reach.\n",
        "    eps_decay: float, default=0.995\n",
        "        The decay rate for epsilon after each episode.\n",
        "    target_update: int, default=10\n",
        "        The frequency (number of episodes) with which the target network should be updated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of float\n",
        "        The total reward obtained in each episode.\n",
        "    '''\n",
        "\n",
        "    # Initialize the scores list and scores window\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    # Loop over episodes\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "\n",
        "        # Reset environment and score at the start of each episode\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        # Loop over steps\n",
        "        while True:\n",
        "\n",
        "            # Select an action using current agent policy then apply in environment\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update the agent, state and score\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # End the episode if done\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # At the end of episode append and save scores\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "\n",
        "        # Decrease epsilon\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        # Print some info\n",
        "        print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\", end=\"\")\n",
        "\n",
        "        # Update target network every target_update episodes\n",
        "        if i_episode % target_update == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        # Print average score every 100 episodes\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        # This environment is considered to be solved for a mean score of 200 or greater, so stop training.\n",
        "        if i_episode % 100 == 0 and np.mean(scores_window) >= 200:\n",
        "            break\n",
        "\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v3')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a DQN agent\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env, 2000)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T07:45:28.750092Z",
          "iopub.execute_input": "2025-02-26T07:45:28.750370Z",
          "iopub.status.idle": "2025-02-26T07:46:55.471185Z",
          "shell.execute_reply.started": "2025-02-26T07:45:28.750351Z",
          "shell.execute_reply": "2025-02-26T07:46:55.470446Z"
        },
        "trusted": true,
        "id": "SORJ5lJXfZy7",
        "outputId": "2c99c637-f314-43ab-bc6d-2338e097ec41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -157.75\n",
            "Episode 200\tAverage Score: -55.66\n",
            "Episode 300\tAverage Score: -28.26\n",
            "Episode 309\tAverage Score: -25.02"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "- Our DQN agent is able to solve the game typically after playing around 1200 episodes.\n",
        "- Let's watch a video of this agent's performance:"
      ],
      "metadata": {
        "id": "N--ngA0wfZy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v3',render_mode='rgb_array')\n",
        "\n",
        "def play_DQN_episode(env, agent):\n",
        "    score = 0\n",
        "    state, _ = env.reset(seed=42)\n",
        "\n",
        "    frames=[]\n",
        "\n",
        "    while True:\n",
        "        # eps=0 for predictions\n",
        "        frames.append(env.render())\n",
        "        action = agent.act(state, 0)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        # End the episode if done\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return score\n",
        "\n",
        "display_video_from_frames(frames,video_filename='after_training.mp4')\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T07:46:55.472106Z",
          "iopub.execute_input": "2025-02-26T07:46:55.472333Z",
          "iopub.status.idle": "2025-02-26T07:46:56.416567Z",
          "shell.execute_reply.started": "2025-02-26T07:46:55.472315Z",
          "shell.execute_reply": "2025-02-26T07:46:56.415754Z"
        },
        "id": "zRc5PRthfZy8",
        "outputId": "73871b0f-9349-46c8-9710-43d693d5f4e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video after_training.mp4.\n",
            "Moviepy - Writing video after_training.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready after_training.mp4\n",
            "Score obtained: -56.45490875162482\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7PrmYC4wfZy9"
      }
    }
  ]
}
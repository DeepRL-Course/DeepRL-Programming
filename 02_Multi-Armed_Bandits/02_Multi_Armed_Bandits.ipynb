{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "======================================================================================================\n",
        "\n",
        "Disclaimer: Parts of this notebook are adapted from [this](https://github.com/DeepRLCourse/Workshop-6-Material/blob/main/Workshop6_Notebook.ipynb) GitHub repo with minor modifications. Credit to the original authors.\n",
        "\n",
        "======================================================================================================"
      ],
      "metadata": {
        "id": "-FJM_ur9HwDs"
      },
      "id": "-FJM_ur9HwDs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3YDSMAiek78"
      },
      "source": [
        "# Workshop 2 : Multi-Armed Bandits\n",
        "\n",
        "In this notebook, we will explore the core concepts of **multi-armed bandits**, focusing on:\n",
        "\n",
        "1. Definitions and properties.\n",
        "2. Action-value methods and uncertainty in bandits.\n",
        "3. Exploration-exploitation strategy:\n",
        "   - $\\epsilon$-greedy\n",
        "\n",
        "4. Real-world applications:\n",
        "   - Recommender Systems\n",
        "   - Search\n",
        "\n",
        "We'll show **pure Python implementations** and **Gymnasium-based** implementations, so you can see how each algorithm might look in a more standardized RL environment setup.\n",
        "\n",
        "\n",
        "> **Note:** In typical RL frameworks, a bandit is treated as either a single-step environment (reset after each pull) or a multi-step environment with a fixed horizon. The examples below choose a **multi-step environment with a fixed horizon** to mimic `num_steps` pulls in a single episode.\n",
        "\n",
        "\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "- Understand the **multi-armed bandit problem** and how it differs from full RL (non-associative property).\n",
        "- Implement the exploration method of ($\\epsilon$-greedy), both purely and via Gymnasium.\n",
        "- See how these ideas apply to **recommender systems** and **search**."
      ],
      "id": "k3YDSMAiek78"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ZQXoj6ek7-"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "We'll import:\n",
        "- `numpy` for numerical operations\n",
        "- `matplotlib` for plotting\n",
        "- `gymnasium` to demonstrate how to wrap the bandit environment in the standard Gym interface\n",
        "\n",
        "We also set a random seed for reproducibility."
      ],
      "id": "S-ZQXoj6ek7-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9e5RHUYek7_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "x9e5RHUYek7_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llhVKi7Gek7_"
      },
      "source": [
        "## 2. Multi-Armed Bandits: Recap\n",
        "\n",
        "### 2.1 Definition of the Problem and Non-Associativity Property\n",
        "\n",
        "\n",
        "A **multi-armed bandit (MAB)** is a fundamental reinforcement learning setting modeling **decision-making under uncertainty**. The problem is often described as having $k$ different slot machines (arms), each with an **unknown** distribution of rewards.\n",
        "\n",
        "- We have **$ k $** arms/actions.\n",
        "- Each arm $ a $ has an **unknown expected reward** $ q_*(a) $:\n",
        "  $$\n",
        "  q_*(a) = \\mathbb{E}[R \\mid A = a],\n",
        "  $$\n",
        "  where $ R $ is the reward for choosing action $ A $.\n",
        "- We want to **maximize the cumulative reward**:\n",
        "  $$\n",
        "  G = \\sum_{t=1}^{T} R_t.\n",
        "  $$\n",
        "- We **do not** know $ q_*(a) $ and must **estimate** these values over time.\n",
        "\n",
        "**Non-Associativity**:\n",
        "- Unlike a full RL problem, multi-armed bandits are **non-associative**â€”there are no distinct states or transitions.\n",
        "- The best action does not depend on any \"state\"; the challenge is figuring out which action yields the highest expected reward overall."
      ],
      "id": "llhVKi7Gek7_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRvHxXMfek7_"
      },
      "source": [
        "### 2.2 Action-Value Methods and Types\n",
        "\n",
        "\n",
        "We define the **action-value** $ Q_t(a) $ as our estimate of $ q_*(a) $. Using **sample-average methods**:\n",
        "$$\n",
        "Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i,\n",
        "$$\n",
        "where $ N_t(a) $ is the number of times action $ a $ has been chosen up to time $ t $, and $ R_i $ are the observed rewards.\n",
        "\n",
        "**Incremental Update Rule** (to avoid storing entire histories):\n",
        "$$\n",
        "Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)}(R_t - Q_t(a)).\n",
        "$$\n",
        "\n",
        "#### Constant Step-Size for Nonstationary Problems\n",
        "When rewards change over time, we use **constant step-size** $ \\alpha $:\n",
        "$$\n",
        "Q_{t+1}(a) = Q_t(a) + \\alpha (R_t - Q_t(a)).\n",
        "$$\n",
        "- Large $\\alpha$ gives more weight to recent data, useful for **nonstationary** environments."
      ],
      "id": "WRvHxXMfek7_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCA-sLOXek8A"
      },
      "source": [
        "### 2.3 Exploration-Exploitation Dilemma and Uncertainty\n",
        "\n",
        "\n",
        "A core challenge is balancing **exploration** and **exploitation**:\n",
        "- **Exploration**: try different actions to reduce uncertainty in their value estimates.\n",
        "- **Exploitation**: select the best-known action to maximize immediate reward.\n",
        "\n",
        "Mathematically, if an action is not selected often, $ N_t(a) $ remains small, so the **uncertainty** in its estimate is large. An agent must ensure all actions are sufficiently explored to accurately approximate $ q_*(a) $."
      ],
      "id": "lCA-sLOXek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rdx_cQmek8A"
      },
      "source": [
        "## 3. Exploration in Bandits\n",
        "\n",
        "We now discuss the exploration strategies in MAB. We'll first show **plain Python** implementations, then **Gymnasium-based** versions."
      ],
      "id": "3rdx_cQmek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjV7RzUWek8A"
      },
      "source": [
        "### 3.1 $\\epsilon$-Greedy\n",
        "\n",
        "From the recitation:\n",
        "\n",
        "> **$\\epsilon$-greedy**:\n",
        "> $$\n",
        "> A_t =\n",
        "> \\begin{cases}\n",
        "> \\arg\\max_a Q_t(a), & \\text{with probability } 1-\\epsilon \\\\\n",
        "> \\text{random } a, & \\text{with probability } \\epsilon.\n",
        "> \\end{cases}\n",
        "> $$\n",
        "\n",
        "- Simple to implement.\n",
        "- Guarantees each arm is sampled (with probability $\\epsilon$).\n",
        "- Can be inefficient if $\\epsilon$ is large because it spends too much time on suboptimal arms.\n",
        "\n",
        "#### 3.1.1 Plain Python: Bernoulli Bandit and $\\epsilon$-Greedy"
      ],
      "id": "CjV7RzUWek8A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZncU5qek8A",
        "outputId": "4bc19793-9b91-48a3-cc10-41e3d8581d39"
      },
      "source": [
        "class BernoulliBandit:\n",
        "    \"\"\"\n",
        "    A simple k-armed Bernoulli bandit.\n",
        "    Each arm i has a probability p[i] of returning reward=1.\n",
        "    \"\"\"\n",
        "    def __init__(self, p):\n",
        "        self.p = np.array(p)\n",
        "        self.k = len(p)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Returns 1 with probability p[action], else 0\n",
        "        return 1 if np.random.rand() < self.p[action] else 0\n",
        "\n",
        "def epsilon_greedy(bandit, num_steps=1000, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy action selection.\n",
        "    bandit: an instance of BernoulliBandit\n",
        "    num_steps: total number of pulls\n",
        "    epsilon: exploration rate\n",
        "    \"\"\"\n",
        "    k = bandit.k\n",
        "    Q = np.zeros(k) # Estimated values of arms\n",
        "    N = np.zeros(k) # Count of pulls for each arm\n",
        "\n",
        "    rewards_history = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        # Explore vs Exploit\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k) # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q) # Exploit\n",
        "\n",
        "        reward = bandit.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action] # Update estimate\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (plain Python)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "bandit = BernoulliBandit(p)\n",
        "rewards, Q_est = epsilon_greedy(bandit, num_steps=1000, epsilon=0.1)\n",
        "print(f\"Estimated Q-values: {Q_est}\")\n",
        "print(f\"Total reward: {rewards.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Q-values: [0.0625     0.36585366 0.80241493]\n",
            "Total reward: 749 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "S-ZncU5qek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuPNSFU_ek8A"
      },
      "source": [
        "#### 3.1.2 Gymnasium-Based Implementation: $\\epsilon$-Greedy\n",
        "\n",
        "Below, we wrap the Bernoulli bandit logic in a Gymnasium environment. In this approach:\n",
        "- We have a single episode of length `num_steps`.\n",
        "- The environmentâ€™s `action_space` is `Discrete(k)`.\n",
        "- Observations are trivial (weâ€™ll just return a dummy array).\n",
        "- After each `step(action)`, we increment an internal step count and eventually set `done=True` when `num_steps` is reached.\n",
        "\n",
        "Then we run an $\\epsilon$-greedy policy in the **standard Gym loop**."
      ],
      "id": "XuPNSFU_ek8A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jsu6xSyek8A",
        "outputId": "45286cd9-84a8-4e82-88a3-db0ffad5b96f"
      },
      "source": [
        "class BernoulliBanditEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for a k-armed Bernoulli bandit.\n",
        "    The episode length is fixed to num_steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, p, num_steps=1000):\n",
        "        super().__init__()\n",
        "        self.p = np.array(p)\n",
        "        self.k = len(p)\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        # Define action & observation space\n",
        "        self.action_space = spaces.Discrete(self.k)\n",
        "        # We'll return a dummy observation (e.g., shape (1,))\n",
        "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        # Return a dummy observation\n",
        "        return np.array([0.0], dtype=np.float32), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 1 if np.random.rand() < self.p[action] else 0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_steps)\n",
        "        # We don't have a meaningful observation, so just return a dummy\n",
        "        obs = np.array([0.0], dtype=np.float32)\n",
        "        info = {}\n",
        "        return obs, float(reward), done, False, info\n",
        "\n",
        "# Gym-based epsilon-greedy\n",
        "def epsilon_greedy_gym(env, epsilon=0.1):\n",
        "    k = env.action_space.n\n",
        "    Q = np.zeros(k)\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    step_count = 0\n",
        "\n",
        "    while not done:\n",
        "        step_count += 1\n",
        "        # Explore vs Exploit\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (Gym)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "env = BernoulliBanditEnv(p, num_steps=1000)\n",
        "gym_rewards, gym_Q = epsilon_greedy_gym(env, epsilon=0.1)\n",
        "print(f\"[GYM] Estimated Q-values: {gym_Q}\")\n",
        "print(f\"[GYM] Total reward: {gym_rewards.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM] Estimated Q-values: [0.0625     0.33962264 0.80294451]\n",
            "[GYM] Total reward: 731.0 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "4Jsu6xSyek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc0YcEOAek8D"
      },
      "source": [
        "## 4. Bandit Applications\n",
        "In this section, we provide **practical examples** for two major real-world applications of bandits:\n",
        "- Recommender Systems\n",
        "- Search\n",
        "\n",
        "Weâ€™ll show both **pure Python** and **Gymnasium** approaches."
      ],
      "id": "Rc0YcEOAek8D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokDaD3Fek8D"
      },
      "source": [
        "### 4.1 Recommender Systems\n",
        "\n",
        "A recommender system often picks an item (action) to recommend for a particular user, receives feedback (reward), and aims to maximize user satisfaction (clicks, watch time, etc.).\n",
        "\n",
        "#### 4.1.1 Pure Python Example [EXTRA]\n",
        "We'll simulate a scenario where we have `k` items to recommend, and each user is described by a **context** vector. We'll keep it simple:\n",
        "- Each item has a weight vector.\n",
        "- The user context is sampled randomly.\n",
        "- The probability of a click (reward=1) is `sigmoid(w_item . context)`.\n",
        "We'll implement a minimal function to **run an $\\epsilon$-greedy** approach for demonstration."
      ],
      "id": "gokDaD3Fek8D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqbm4UcWek8D",
        "outputId": "e1366282-cae0-47fd-de7c-be0cfafbf6a0"
      },
      "source": [
        "class RecommenderSystemBandit:\n",
        "    \"\"\"\n",
        "    Pure Python bandit environment for a recommender system.\n",
        "    \"\"\"\n",
        "    def __init__(self, item_weights, context_dim=2, num_users=1000):\n",
        "        \"\"\"\n",
        "        item_weights: np.array of shape (k, context_dim)\n",
        "        context_dim: dimension of each user's feature vector\n",
        "        num_users: how many user interactions we simulate\n",
        "        \"\"\"\n",
        "        self.item_weights = item_weights\n",
        "        self.k = item_weights.shape[0]\n",
        "        self.context_dim = context_dim\n",
        "        self.num_users = num_users\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_context(self):\n",
        "        # Sample a random user feature vector\n",
        "        return np.random.randn(self.context_dim)\n",
        "\n",
        "    def step(self, action, user_context):\n",
        "        \"\"\"\n",
        "        Probability of reward = sigmoid(item_weights[action] . user_context)\n",
        "        \"\"\"\n",
        "        w = self.item_weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(user_context)))\n",
        "        reward = 1 if np.random.rand() < p else 0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_users)\n",
        "        return reward, done\n",
        "\n",
        "def run_epsilon_greedy_recommender(env, epsilon=0.1):\n",
        "    k = env.k\n",
        "    Q = np.zeros(k)\n",
        "    # The number of times each 'action' has been selected,\n",
        "    # each element in N corresponds to a particular arm.\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    while True:\n",
        "        context = env.get_context()\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        reward, done = env.step(action, context)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo: RecommenderSystemBandit with k=3 items\n",
        "item_weights = np.array([\n",
        "    [1.5, 0.5],   # item 0\n",
        "    [-0.5, 1.2], # item 1\n",
        "    [0.8, 0.8]   # item 2\n",
        "])\n",
        "rec_env = RecommenderSystemBandit(item_weights, context_dim=2, num_users=1000)\n",
        "recomm_rewards, recomm_Q = run_epsilon_greedy_recommender(rec_env, epsilon=0.1)\n",
        "print(\"Recommender (pure Python) total reward:\", recomm_rewards.sum())\n",
        "print(\"Final Q estimates:\", recomm_Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommender (pure Python) total reward: 476\n",
            "Final Q estimates: [0.46927374 0.48042705 0.47592593]\n"
          ]
        }
      ],
      "id": "Jqbm4UcWek8D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPZRJCgnek8D"
      },
      "source": [
        "#### 4.1.2 Gymnasium Example [EXTRA]\n",
        "We create a Gym environment where each step corresponds to recommending an item for a randomly sampled user context. Weâ€™ll do a short **$\\epsilon$-greedy** run."
      ],
      "id": "FPZRJCgnek8D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8ZsZARTek8E",
        "outputId": "1c494e84-de6c-4334-c5d5-79ae6389ef87"
      },
      "source": [
        "class RecommenderSystemGymEnv(gym.Env):\n",
        "    def __init__(self, item_weights, context_dim=2, num_users=1000):\n",
        "        super().__init__()\n",
        "        self.item_weights = item_weights\n",
        "        self.k = item_weights.shape[0]\n",
        "        self.context_dim = context_dim\n",
        "        self.num_users = num_users\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.k)\n",
        "        # Observation is the user context\n",
        "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(self.context_dim,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        self.context = np.random.randn(self.context_dim).astype(np.float32)\n",
        "        return self.context, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        w = self.item_weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(self.context)))\n",
        "        reward = 1.0 if np.random.rand() < p else 0.0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_users)\n",
        "        info = {}\n",
        "\n",
        "        # sample next user context\n",
        "        self.context = np.random.randn(self.context_dim).astype(np.float32)\n",
        "        return self.context, reward, done, False, info\n",
        "\n",
        "def run_epsilon_greedy_recommender_gym(env, epsilon=0.1):\n",
        "    k = env.action_space.n\n",
        "    Q = np.zeros(k)\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        new_obs, reward, done, truncated, info = env.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        obs = new_obs\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo: RecommenderSystemGymEnv\n",
        "item_weights = np.array([\n",
        "    [1.5, 0.5],   # item 0\n",
        "    [-0.5, 1.2], # item 1\n",
        "    [0.8, 0.8]   # item 2\n",
        "])\n",
        "env_rec = RecommenderSystemGymEnv(item_weights, context_dim=2, num_users=1000)\n",
        "rec_gym_rewards, rec_gym_Q = run_epsilon_greedy_recommender_gym(env_rec, epsilon=0.1)\n",
        "print(\"[GYM Recommender] total reward:\", rec_gym_rewards.sum())\n",
        "print(\"[GYM Recommender] final Q estimates:\", rec_gym_Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM Recommender] total reward: 517.0\n",
            "[GYM Recommender] final Q estimates: [0.51923077 0.52070263 0.44680851]\n"
          ]
        }
      ],
      "id": "D8ZsZARTek8E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ2qVWXCek8E"
      },
      "source": [
        "### 4.2 Search\n",
        "\n",
        "A simplified \"search\" bandit scenario might involve multiple **ranking strategies** or **candidate documents**. We pick which strategy or document to display for a given query, and get a click/no-click reward. Below, we provide a toy model:\n",
        "- We have `k` different \"search strategies.\"\n",
        "- Each query is a context vector.\n",
        "- Probability of a click is a logistic function of the chosen strategyâ€™s weight.\n",
        "\n",
        "#### 4.2.1 Pure Python Example [EXTRA]\n",
        "We'll do the same structure as the Recommender, but weâ€™ll call it \"SearchBandit.\""
      ],
      "id": "QZ2qVWXCek8E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68PWkTktek8E",
        "outputId": "8e94d334-5f67-4a35-e9b3-c92688653a04"
      },
      "source": [
        "class SearchBandit:\n",
        "    \"\"\"\n",
        "    Pure Python environment to simulate 'search strategies' bandit.\n",
        "    \"\"\"\n",
        "    def __init__(self, strategy_weights, context_dim=2, num_queries=500):\n",
        "        self.strategy_weights = strategy_weights\n",
        "        self.k = strategy_weights.shape[0]\n",
        "        self.context_dim = context_dim\n",
        "        self.num_queries = num_queries\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_context(self):\n",
        "        return np.random.randn(self.context_dim)\n",
        "\n",
        "    def step(self, action, query_context):\n",
        "        w = self.strategy_weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(query_context)))\n",
        "        reward = 1 if np.random.rand() < p else 0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_queries)\n",
        "        return reward, done\n",
        "\n",
        "def run_thompson_search(env):\n",
        "    \"\"\"\n",
        "    We'll demonstrate Thompson Sampling here.\n",
        "    \"\"\"\n",
        "    k = env.k\n",
        "    alpha = np.ones(k)\n",
        "    beta = np.ones(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    while True:\n",
        "        query_context = env.get_context()\n",
        "        sampled_q = np.random.beta(alpha, beta)\n",
        "        action = np.argmax(sampled_q)\n",
        "        reward, done = env.step(action, query_context)\n",
        "        if reward == 1:\n",
        "            alpha[action] += 1\n",
        "        else:\n",
        "            beta[action] += 1\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), alpha, beta\n",
        "\n",
        "# Demo: 3 search strategies\n",
        "strategy_weights = np.array([\n",
        "    [1.0, 1.0],   # strategy 0\n",
        "    [-0.5, 2.0], # strategy 1\n",
        "    [2.0, -0.2]  # strategy 2\n",
        "])\n",
        "search_env = SearchBandit(strategy_weights, context_dim=2, num_queries=500)\n",
        "search_rewards, alpha_s, beta_s = run_thompson_search(search_env)\n",
        "print(\"Search (pure Python) total reward:\", search_rewards.sum())\n",
        "print(\"Alpha:\", alpha_s, \"Beta:\", beta_s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search (pure Python) total reward: 264\n",
            "Alpha: [114.  68.  85.] Beta: [98. 69. 72.]\n"
          ]
        }
      ],
      "id": "68PWkTktek8E"
    }
  ]
}